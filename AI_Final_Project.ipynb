{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9ac3d5a",
      "metadata": {
        "id": "d9ac3d5a"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "bb821edd",
      "metadata": {
        "id": "bb821edd"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d616f1d0",
      "metadata": {
        "id": "d616f1d0"
      },
      "source": [
        "* Make a move for X -- pick best move, train the model based on picked state and associated reward, update state\n",
        "* determine best move -- based on Q grid\n",
        "* Train Model -- x value is the state as an array, y value (target) is the q-value of best next move + reward at current state \n",
        "* Calc_value_of_state -- use model to predict value of state\n",
        "* calc target -- is the q-value of best next move + reward at current state \n",
        "* run experiment -- runs through a tictactoe game "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16201813",
      "metadata": {
        "id": "16201813"
      },
      "source": [
        "# Piece Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "d87979f2",
      "metadata": {
        "id": "d87979f2"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Piece(Enum):\n",
        "    EMPTY = 'EMPTY'\n",
        "    BLACK = 'BLACK'\n",
        "    WHITE = 'WHITE'\n",
        "    RED = ' RED '"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23c6c04",
      "metadata": {
        "id": "d23c6c04"
      },
      "source": [
        "# Board Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "c95e6981",
      "metadata": {
        "id": "c95e6981"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class Board:\n",
        "    def __init__(self):\n",
        "        self.pieces = [[[Piece.EMPTY for k in range(3)] for j in range(3)] for i in range(3)]\n",
        "        self.winningRuns = self.getWinningRuns()\n",
        "\n",
        "    def validMove(self,x,y,z,dir):\n",
        "        if not x in range(3) or not y in range(3) or not z in range(3):\n",
        "              return False\n",
        "        if dir == 'UP':\n",
        "              return (z == 2) and (self.pieces[x][y][z] == Piece.EMPTY or self.pieces[x][y][z-1] == Piece.EMPTY or self.pieces[x][y][z-2] == Piece.EMPTY)\n",
        "        if dir == 'DOWN':\n",
        "              return (z == 0) and (self.pieces[x][y][z] == Piece.EMPTY or self.pieces[x][y][z+1] == Piece.EMPTY or self.pieces[x][y][z+2] == Piece.EMPTY)\n",
        "        if dir == 'LEFT':\n",
        "              return (x == 2) and (self.pieces[x][y][z] == Piece.EMPTY or self.pieces[x-1][y][z] == Piece.EMPTY or self.pieces[x-2][y][z] == Piece.EMPTY)\n",
        "        if dir == 'RIGHT':\n",
        "              return (x == 0) and (self.pieces[x][y][z] == Piece.EMPTY or self.pieces[x+1][y][z] == Piece.EMPTY or self.pieces[x+2][y][z] == Piece.EMPTY)\n",
        "        if dir == 'FRONT':\n",
        "              return (y == 2) and (self.pieces[x][y][z] == Piece.EMPTY or self.pieces[x][y-1][z] == Piece.EMPTY or self.pieces[x][y-2][z] == Piece.EMPTY)\n",
        "        if dir == 'BACK':\n",
        "              return (y == 0) and (self.pieces[x][y][z] == Piece.EMPTY or self.pieces[x][y+1][z] == Piece.EMPTY or self.pieces[x][y+2][z] == Piece.EMPTY)\n",
        "        else:\n",
        "              return False\n",
        "            \n",
        "    def move(self,x,y,z,dir,player: Piece):\n",
        "        if not self.validMove(x,y,z,dir):\n",
        "             raise ValueError\n",
        "        else:\n",
        "            if (self.pieces[x][y][z] == Piece.EMPTY):\n",
        "                self.pieces[x][y][z] = player\n",
        "            else:\n",
        "                if dir == 'UP':\n",
        "                    if (self.pieces[x][y][z-1] == Piece.EMPTY):\n",
        "                        self.pieces[x][y][z-1] = self.pieces[x][y][z]\n",
        "                        self.pieces[x][y][z] = player\n",
        "                    else:\n",
        "                        self.pieces[x][y][z-2] = self.pieces[x][y][z-1]\n",
        "                        self.pieces[x][y][z-1] = self.pieces[x][y][z]\n",
        "                        self.pieces[x][y][z] = player\n",
        "                elif dir == 'DOWN':\n",
        "                      if (self.pieces[x][y][z+1] == Piece.EMPTY):\n",
        "                          self.pieces[x][y][z+1] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                      else:\n",
        "                          self.pieces[x][y][z+2] = self.pieces[x][y][z+1]\n",
        "                          self.pieces[x][y][z+1] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                elif dir == 'LEFT':\n",
        "                      if (self.pieces[x-1][y][z] == Piece.EMPTY):\n",
        "                          self.pieces[x-1][y][z] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                      else:\n",
        "                          self.pieces[x-2][y][z] = self.pieces[x-1][y][z]\n",
        "                          self.pieces[x-1][y][z] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                elif dir == 'RIGHT':\n",
        "                      if (self.pieces[x+1][y][z] == Piece.EMPTY):\n",
        "                          self.pieces[x+1][y][z] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                      else:\n",
        "                          self.pieces[x+2][y][z] = self.pieces[x+1][y][z]\n",
        "                          self.pieces[x+1][y][z] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                elif dir == 'FRONT':\n",
        "                      if (self.pieces[x][y-1][z] == Piece.EMPTY):\n",
        "                          self.pieces[x][y-1][z] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                      else:\n",
        "                          self.pieces[x][y-2][z] = self.pieces[x][y-1][z]\n",
        "                          self.pieces[x][y-1][z] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                elif dir == 'BACK':\n",
        "                      if (self.pieces[x][y+1][z] == Piece.EMPTY):\n",
        "                          self.pieces[x][y+1][z] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "                      else:\n",
        "                          self.pieces[x][y+2][z] = self.pieces[x][y+1][z]\n",
        "                          self.pieces[x][y+1][z] = self.pieces[x][y][z]\n",
        "                          self.pieces[x][y][z] = player\n",
        "\n",
        "    def getGameState(self):\n",
        "        gameState = \"+----------------------\\n\"\n",
        "        gameState += \"| \\ \" + self.pieces[0][2][0].value + \"  \" + self.pieces[1][2][0].value + \"  \" + self.pieces[2][2][0].value + \" \\\\\\n\"\n",
        "        gameState += \"|   \\                     \\\\\\n\"\n",
        "        gameState += \"|     \\ \" + self.pieces[0][1][0].value + \"  \" + self.pieces[1][1][0].value + \"  \" + self.pieces[2][1][0].value + \" \\\\\\n\"\n",
        "        gameState += \"|       \\                     \\\\\\n\"\n",
        "        gameState += \"|         \\ \" + self.pieces[0][0][0].value + \"  \" + self.pieces[1][0][0].value + \"  \" + self.pieces[2][0][0].value + \" \\\\\\n\"\n",
        "        gameState += \"|          ---------------------|\\n\"\n",
        "        gameState += \"|   \" + self.pieces[0][2][1].value + \" |\" + self.pieces[1][2][1].value + \"  \" + self.pieces[2][2][1].value + \"         |\\n\"\n",
        "        gameState += \"|         |                     |\\n\"\n",
        "        gameState += \"|       \" + self.pieces[0][1][1].value + \"  \" + self.pieces[1][1][1].value + \"  \" + self.pieces[2][1][1].value + \"     |\\n\"\n",
        "        gameState += \"|         |                     |\\n\"\n",
        "        gameState += \"|         | \" + self.pieces[0][0][1].value + \"  \" + self.pieces[1][0][1].value + \"  \" + self.pieces[2][0][1].value + \" |\\n\"\n",
        "        gameState += \"|         |                     |\\n\"\n",
        "        gameState += \" \\ \" + self.pieces[0][2][2].value + \"  \" + self.pieces[1][2][2].value + \"  \" + self.pieces[2][2][2].value + \"          |\\n\"\n",
        "        gameState += \"   \\      |                     |\\n\"\n",
        "        gameState += \"     \\ \" + self.pieces[0][1][2].value + \"  \" + self.pieces[1][1][2].value + \"  \" + self.pieces[2][1][2].value + \"      |\\n\"\n",
        "        gameState += \"       \\  |                     |\\n\"\n",
        "        gameState += \"         \\| \" + self.pieces[0][0][2].value + \"  \" + self.pieces[1][0][2].value + \"  \" + self.pieces[2][0][2].value + \" |\\n\"\n",
        "        gameState += \"           ---------------------+\\n\\n\"\n",
        "        return gameState\n",
        "    \n",
        "\n",
        "    def getWinningRuns(self):\n",
        "        runs = []\n",
        "\n",
        "        runs.append([(0,0,0),(0,0,1),(0,0,2)])\n",
        "        runs.append([(0,0,0),(0,1,0),(0,2,0)])\n",
        "        runs.append([(0,0,0),(1,0,0),(2,0,0)])\n",
        "\n",
        "        runs.append([(2,2,0),(1,2,0),(0,2,0)])\n",
        "        runs.append([(2,2,0),(2,1,0),(2,0,0)])\n",
        "        runs.append([(2,2,0),(2,2,1),(2,2,2)])\n",
        "\n",
        "        runs.append([(0,2,2),(0,1,2),(0,0,2)])\n",
        "        runs.append([(0,2,2),(1,2,2),(2,2,2)])\n",
        "        runs.append([(0,2,2),(0,2,1),(0,2,0)])\n",
        "\n",
        "        runs.append([(2,0,2),(2,0,1),(2,0,0)])\n",
        "        runs.append([(2,0,2),(1,0,2),(0,0,2)])\n",
        "        runs.append([(2,0,2),(2,1,2),(2,2,2)])\n",
        "        # Front\n",
        "        runs.append([(0,0,0),(1,0,1),(2,0,2)])\n",
        "        runs.append([(0,0,2),(1,0,1),(2,0,0)])\n",
        "        runs.append([(1,0,0),(1,0,1),(1,0,2)])\n",
        "        runs.append([(0,0,1),(1,0,1),(2,0,1)])\n",
        "        # Top\n",
        "        runs.append([(0,0,0),(1,1,0),(2,2,0)])\n",
        "        runs.append([(0,2,0),(1,1,0),(2,0,0)])\n",
        "        runs.append([(0,1,0),(1,1,0),(2,1,0)])\n",
        "        runs.append([(1,2,0),(1,1,0),(1,0,0)])\n",
        "        # Left\n",
        "        runs.append([(0,0,0),(0,1,1),(0,2,2)])\n",
        "        runs.append([(0,0,2),(0,1,1),(0,2,0)])\n",
        "        runs.append([(0,0,1),(0,1,1),(0,2,1)])\n",
        "        runs.append([(0,1,0),(0,1,1),(0,1,2)])\n",
        "        # Back\n",
        "        runs.append([(0,2,2),(1,2,1),(2,2,0)])\n",
        "        runs.append([(0,2,0),(1,2,1),(2,2,2)])\n",
        "        runs.append([(1,2,0),(1,2,1),(1,2,2)])\n",
        "        runs.append([(0,2,1),(1,2,1),(2,2,1)])\n",
        "        # Right\n",
        "        runs.append([(2,0,2),(2,1,1),(2,2,0)])\n",
        "        runs.append([(2,0,0),(2,1,1),(2,2,2)])\n",
        "        runs.append([(2,0,1),(2,1,1),(2,2,1)])\n",
        "        runs.append([(2,1,0),(2,1,1),(2,1,2)])\n",
        "        # Bottom\n",
        "        runs.append([(2,0,2),(1,1,2),(0,2,2)])\n",
        "        runs.append([(0,0,2),(1,1,2),(2,2,2)])\n",
        "        runs.append([(0,1,2),(1,1,2),(2,1,2)])\n",
        "        runs.append([(1,0,2),(1,1,2),(1,2,2)])\n",
        "        # Corners\n",
        "        runs.append([(0,0,0),(1,1,1),(2,2,2)])\n",
        "        runs.append([(2,0,0),(1,1,1),(0,2,2)])\n",
        "        runs.append([(2,2,0),(1,1,1),(0,0,2)])\n",
        "        runs.append([(0,2,0),(1,1,1),(2,0,2)])\n",
        "        # Edges\n",
        "        runs.append([(1,0,0),(1,1,1),(1,2,2)])\n",
        "        runs.append([(2,1,0),(1,1,1),(0,1,2)])\n",
        "        runs.append([(1,2,0),(1,1,1),(1,0,2)])\n",
        "        runs.append([(0,1,0),(1,1,1),(2,1,2)])\n",
        "        runs.append([(0,0,1),(1,1,1),(2,2,1)])\n",
        "        runs.append([(2,0,1),(1,1,1),(0,2,1)])\n",
        "        # Middles\n",
        "        runs.append([(1,1,0),(1,1,1),(1,1,2)])\n",
        "        runs.append([(1,0,1),(1,1,1),(1,2,1)])\n",
        "        runs.append([(0,1,1),(1,1,1),(2,1,1)])\n",
        "\n",
        "        return runs\n",
        "\n",
        "    def getPossibleMoves(self):\n",
        "        directions = ['UP','DOWN','LEFT','RIGHT','FRONT','BACK']\n",
        "        moves = []\n",
        "        for x in range(2):\n",
        "          for y in range(2):\n",
        "            for z in range(2):\n",
        "              for dir in directions:\n",
        "                if self.validMove(x,y,z,dir):\n",
        "                  moves.append((x,y,z,dir))\n",
        "        return moves\n",
        "\n",
        "    def getWinInOne(self,player: Piece):\n",
        "        for (x,y,z,dir) in self.getPossibleMoves():\n",
        "          c = copy.deepcopy(self)\n",
        "          c.move(x,y,z,dir,player)\n",
        "          if c.hasWon(player):\n",
        "            return (x,y,z,dir)\n",
        "        return None\n",
        "\n",
        "    def otherPlayer(self,player: Piece):\n",
        "        return Piece.RED if player == Piece.WHITE else Piece.WHITE\n",
        "\n",
        "    def getDefendingMove(self,player: Piece):\n",
        "        potential_moves = []\n",
        "        for (x,y,z,dir) in self.getPossibleMoves():\n",
        "          c = copy.deepcopy(self)\n",
        "          c.move(x,y,z,dir,player)\n",
        "          if c.getWinInOne(self.otherPlayer(player)) == None:\n",
        "            potential_moves.append((x,y,z,dir))\n",
        "        if potential_moves:\n",
        "          return random.choice(potential_moves)\n",
        "        return None\n",
        "\n",
        "    def getWinInTwo(self,player: Piece):\n",
        "        potential_moves = []\n",
        "        for (x,y,z,dir) in self.getPossibleMoves():\n",
        "          c = copy.deepcopy(self)\n",
        "          c.move(x,y,z,dir,player)\n",
        "          winner = True\n",
        "          for (x,y,z,dir) in c.getPossibleMoves():\n",
        "            c2 = copy.deepcopy(c)\n",
        "            c2.move(x,y,z,dir,self.otherPlayer(player))\n",
        "            if not c2.getWinInOne(player):\n",
        "              winner = False\n",
        "          if winner:\n",
        "            potential_moves.append((x,y,z,dir))\n",
        "        if potential_moves:\n",
        "          return random.choice(potential_moves)\n",
        "        return None\n",
        "\n",
        "    def getRandomMove(self,player: Piece):\n",
        "        directions = ['UP','DOWN','LEFT','RIGHT','FRONT','BACK']\n",
        "        x = random.randint(0,2)\n",
        "        y = random.randint(0,2)\n",
        "        z = random.randint(0,2)\n",
        "        dir = random.choice(directions)\n",
        "        while not self.validMove(x,y,z,dir):\n",
        "            x = random.randint(0,2)\n",
        "            y = random.randint(0,2)\n",
        "            z = random.randint(0,2)\n",
        "            dir = random.choice(directions)\n",
        "        return (x,y,z,dir)\n",
        "\n",
        "    def hasWon(self,player: Piece):\n",
        "        for run in self.winningRuns:\n",
        "            if all(self.pieces[x][y][z] == player for (x,y,z) in run):\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    def gameOver(self):\n",
        "        return self.hasWon(Piece.RED) or self.hasWon(Piece.WHITE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90742e7",
      "metadata": {
        "id": "c90742e7"
      },
      "source": [
        "# Random Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "919b0d5d",
      "metadata": {
        "id": "919b0d5d"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self,player):\n",
        "        self.player = player\n",
        "\n",
        "    def getMove(self, board: Board):\n",
        "        return board.getRandomMove(self.player)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Easy Agent"
      ],
      "metadata": {
        "id": "FNIKVDWcIZXz"
      },
      "id": "FNIKVDWcIZXz"
    },
    {
      "cell_type": "code",
      "source": [
        "class EasyAgent:\n",
        "    def __init__(self,player):\n",
        "        self.player = player\n",
        "\n",
        "    def getMove(self, board: Board):\n",
        "        winningMove = board.getWinInOne(self.player)\n",
        "        if winningMove:\n",
        "          return winningMove\n",
        "        else:\n",
        "          return board.getRandomMove(self.player)"
      ],
      "metadata": {
        "id": "oiGGigmkIdv_"
      },
      "id": "oiGGigmkIdv_",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medium Agent"
      ],
      "metadata": {
        "id": "9JkgzsI3uMcs"
      },
      "id": "9JkgzsI3uMcs"
    },
    {
      "cell_type": "code",
      "source": [
        "class MediumAgent:\n",
        "    def __init__(self,player):\n",
        "        self.player = player\n",
        "\n",
        "    def getMove(self, board: Board):\n",
        "        winningMove = board.getWinInOne(self.player)\n",
        "        if winningMove:\n",
        "          return winningMove\n",
        "        else:\n",
        "          defendingMove = board.getDefendingMove(self.player)\n",
        "          if defendingMove:\n",
        "            return defendingMove\n",
        "          else:\n",
        "            return board.getRandomMove(self.player)"
      ],
      "metadata": {
        "id": "0StiPGknuITm"
      },
      "id": "0StiPGknuITm",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hard Agent"
      ],
      "metadata": {
        "id": "9ho8XxlS13pv"
      },
      "id": "9ho8XxlS13pv"
    },
    {
      "cell_type": "code",
      "source": [
        "class HardAgent:\n",
        "    def __init__(self,player):\n",
        "        self.player = player\n",
        "\n",
        "    def getMove(self, board: Board):\n",
        "        winningMove = board.getWinInOne(self.player)\n",
        "        if winningMove:\n",
        "          return winningMove\n",
        "        else:\n",
        "          winInTwo = board.getWinInTwo(self.player)\n",
        "          if winInTwo:\n",
        "            return winInTwo\n",
        "          else:\n",
        "            defendingMove = board.getDefendingMove(self.player)\n",
        "            if defendingMove:\n",
        "              return defendingMove\n",
        "            else:\n",
        "              return board.getRandomMove(self.player)"
      ],
      "metadata": {
        "id": "krHzapzO16Oc"
      },
      "id": "krHzapzO16Oc",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GamePlayer:\n",
        "    def __init__(self,player1,player2):\n",
        "      self.player1 = player1\n",
        "      self.player2 = player2\n",
        "      self.board = Board()\n",
        "\n",
        "    def playGame(self):\n",
        "       self.board = Board()\n",
        "       while 1 == 1:\n",
        "          # Player 1 moves\n",
        "          (x1,y1,z1,dir1) = self.player1.getMove(self.board)\n",
        "          self.board.move(x1,y1,z1,dir1,Piece.RED)\n",
        "          if self.board.hasWon(Piece.RED):\n",
        "             return 'RED'\n",
        "          # Player 2 moves\n",
        "          (x2,y2,z2,dir2) = self.player2.getMove(self.board)\n",
        "          self.board.move(x2,y2,z2,dir2,Piece.WHITE)\n",
        "          if self.board.hasWon(Piece.WHITE):\n",
        "             return 'WHITE'"
      ],
      "metadata": {
        "id": "yQkDHXTzCgTq"
      },
      "id": "yQkDHXTzCgTq",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "150be10c",
      "metadata": {
        "id": "150be10c"
      },
      "source": [
        "# Main Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "194f2e2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "194f2e2e",
        "outputId": "15e54051-7754-450e-91de-9bde5c510e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [06:36<00:00,  3.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red player wins 24% of the time!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [06:30<00:00,  3.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red player wins 93% of the time!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    game = GamePlayer(MediumAgent(Piece.RED),EasyAgent(Piece.WHITE))\n",
        "    red_wins = 0\n",
        "    for i in tqdm(range(100)):\n",
        "      winner = game.playGame()\n",
        "      if winner == 'RED':\n",
        "        red_wins += 1\n",
        "    print(f'Red player wins {red_wins}% of the time!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b774e5a6",
      "metadata": {
        "id": "b774e5a6"
      },
      "source": [
        "# Deep Learning Agent "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2852c60a",
      "metadata": {
        "id": "2852c60a"
      },
      "source": [
        "## Method we might need \n",
        "* Make a move for X -- pick best move, train the model based on picked state and associated reward, update state\n",
        "* determine best move -- based on Q grid\n",
        "* Train Model -- x value is the state as an array, y value (target) is the q-value of best next move + reward at current state \n",
        "* Calc_value_of_state -- use model to predict value of state\n",
        "* calc target -- is the q-value of best next move + reward at current state \n",
        "* run experiment -- runs through a tictactoe game "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71782d0f",
      "metadata": {
        "id": "71782d0f"
      },
      "source": [
        "### Notes\n",
        "The value function determines how good it is to be in state s,\n",
        "Agent can also learn the value of a state-action pair, which is a q value. The q funciton \n",
        "measueres tha value of choosing a particualar action when in a particular state.\n",
        "\n",
        "\n",
        "Deep Q-Learning replaces the regular Q-table with a neural network. Rather than mapping a state-action pair to a q-value, a neural network maps input states to (action, Q-value) pairs.\n",
        "\n",
        "The Bellman equation is a recursive equation that relates the value of a state to the values of its successor states. It decomposes the value function into two parts: an immediate reward and the expected discounted value of the next state. The Bellman equation is used to update the value function iteratively, until it converges to the true value function.\n",
        "\n",
        "https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de52410a",
      "metadata": {
        "id": "de52410a"
      },
      "source": [
        "## Good code references:\n",
        "   * https://github.com/giladariel/TicTacToe_RL/blob/master/DeepTicTacToe_org.py\n",
        "   * https://github.com/mswang12/minDQN/blob/main/minDQN.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea671a2",
      "metadata": {
        "id": "4ea671a2"
      },
      "outputs": [],
      "source": [
        "def initialize_model():\n",
        "    \"\"\" Initializes keras sequential model that will read in state as an array and return list of q values associated\n",
        "    with each possible action. \n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69221f5c",
      "metadata": {
        "id": "69221f5c",
        "outputId": "28daf38b-1809-4494-a4c1-8c3525c33cae"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 9 (3076497746.py, line 10)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[51], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\" Gets the immediate reward of taking an action. Since this is tiktaktoe, if the move\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 9\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "directions = ['UP','DOWN','LEFT','RIGHT','FRONT','BACK']\n",
        "\n",
        "class DeepLearningAgent:\n",
        "    # epsilon initialized to 1 since we start random\n",
        "    def __init__(self,player=Piece.WHITE,epsilon=1, lam=1.0):\n",
        "        self.player = player\n",
        "        self.epsilon = epsilon\n",
        "        self.lam = lam\n",
        "        self.model = initialize_model()\n",
        "        \n",
        "    \"\"\" Gets the immediate reward of taking an action. Since this is tiktaktoe, if the move\n",
        "    resulted in a win, reward is 1, if a loss, reward is -1, if a tie, reward is 0.5, otherwise\n",
        "    the game has not ended and the reward is 0.\n",
        "    \"\"\"\n",
        "    def get_reward(self,board,move):\n",
        "      c = copy.deepcopy(board)\n",
        "      c.move(move)\n",
        "      # Return 1 if the move wins\n",
        "      if c.hasWon(self.player):\n",
        "        return 1\n",
        "      # Returns 0 if the move loses (opponent can win in one)\n",
        "      elif c.getWinInOne(Piece.RED if self.player == Piece.WHITE else Piece.WHITE):\n",
        "        return -1\n",
        "      # Return 0 otherwise\n",
        "      return 0\n",
        "    \n",
        "    def calculate_value():\n",
        "    \"\"\" The value of a specific state is predicted by the model which has been trained\n",
        "    on previously visisted states.\n",
        "    \"\"\"\n",
        "    \n",
        "    def calculate_target():\n",
        "    \"\"\" The target (value estimate of state s) is calculated based on the bellman equation.\n",
        "    The equation combines the immediate reward from the current state and the discounted\n",
        "    value of the best next state. y is discount factor which determines the balance of caring about short \n",
        "    vs long term rewards. Higher value, more weight towards long term rewards. The bellman equation is recursive.\n",
        "    \"\"\"\n",
        "        # target (value of state s) = (reward of state s) + y * (best q value of all possible actions from state s1)\n",
        "        # (best q value of all possible actions from state s1) is calculated by passing s1 into the model\n",
        "\n",
        "    \n",
        "    def train_model():\n",
        "    \"\"\" The model is learning the policy that the agent will use to move around the environment\n",
        "    and choose the best action. Each time the agent decides on an action, the reward (aka the \n",
        "    target) for the state (s1) is calculated. Then using s1 as x and the reward as y, the model\n",
        "    undergoes one iteration of stochastic gradient descent. The output of the model is basically \n",
        "    the models prediction for the q value of the best action. \n",
        "    \"\"\"\n",
        "        \n",
        "        # Use bellman equation (calculate_target()) to get y value\n",
        "        \n",
        "        # train model using model.fit\n",
        "        \n",
        "    def choose_best_move():\n",
        "        \n",
        "        \n",
        "    def play_move():\n",
        "        \n",
        "        # initialize empty replay_memory, as no moves have been made yet\n",
        "        \n",
        "        # if random number is less than epsilon, do random action\n",
        "        \n",
        "        # else use model to predict the best move (model.predict). This will return q value which u need to find\n",
        "        # associated move for \n",
        "        \n",
        "        \n",
        "        # add move to replay_memory. Replay_memory is a list of tuples(state,action,reward,new state)\n",
        "        \n",
        "        # call train model to update model\n",
        "        \n",
        "        # update epsilon \n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412018b1",
      "metadata": {
        "id": "412018b1"
      },
      "source": [
        "# Game Player"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d3c784",
      "metadata": {
        "id": "65d3c784"
      },
      "outputs": [],
      "source": [
        "# from board import Board\n",
        "# from piece import Piece\n",
        "# from random_agent import RandomAgent\n",
        "\n",
        "class GamePlayer:\n",
        "    def __init__(self, player1= DeepLearningAgent(Piece.WHITE), player2= RandomAgent(Piece.RED)):\n",
        "        self.player1 = player1\n",
        "        self.player2 = player2\n",
        "        self.board = Board()\n",
        "\n",
        "#     def playGame(self):\n",
        "#         while 1 == 1:\n",
        "#             # Player 1 moves\n",
        "#             (x1,y1,z1,dir1) = self.player1.getMove(self.board)\n",
        "#             self.board.move(x1,y1,z1,dir1,Piece.WHITE)\n",
        "#             print(self.board.getGameState())\n",
        "#             if self.board.hasWon(Piece.WHITE):\n",
        "#                 print('White wins!')\n",
        "#                 return\n",
        "#             # Player 2 moves\n",
        "#             (x2,y2,z2,dir2) = self.player2.getMove(self.board)\n",
        "#             self.board.move(x2,y2,z2,dir2,Piece.RED)\n",
        "#             print(self.board.getGameState())\n",
        "#             if self.board.hasWon(Piece.RED):\n",
        "#                 print('Red wins!')\n",
        "#                 return\n",
        "\n",
        "        def "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291d6a5b",
      "metadata": {
        "id": "291d6a5b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python310",
      "language": "python",
      "name": "python310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}